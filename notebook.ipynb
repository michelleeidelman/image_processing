{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c67045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy\n",
    "import numpy as np\n",
    "import glob\n",
    "import sns as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import canny\n",
    "from scipy import ndimage as ndi\n",
    "from skimage.filters import sobel\n",
    "from tensorflow.python.keras.utils.version_utils import callbacks\n",
    "\n",
    "width = 128\n",
    "height = 128\n",
    "dim = (width, height)\n",
    "depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c83de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_and_preprocessing():\n",
    "    # create lists to save images and labels (the folder which the image came from)\n",
    "    cv_img = []\n",
    "    cv_img_label = []\n",
    "    # add \"yes\" images to list\n",
    "    for img in glob.glob(os.getcwd() + \"\\\\brain_tumor_dataset\\\\yes\\\\*.jpg\"):\n",
    "        n = cv2.imread(img)\n",
    "        # convert to gray scale\n",
    "        n = cv2.cvtColor(n, cv2.COLOR_BGR2GRAY)\n",
    "        # normalize data to range(0,1).\n",
    "        # In future - the normalization makes convergence faster while training the network.\n",
    "        n = cv2.normalize(n, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        # resize image - change dimensions -> from RGB(3 channels) to gray(1 channel)\n",
    "        resized = cv2.resize(n, dim, interpolation=cv2.INTER_AREA)\n",
    "        # add image after changes to image list\n",
    "        cv_img.append(resized)\n",
    "        # add label 1 (=is brain tumor)\n",
    "        cv_img_label.append(1)\n",
    "    # add \"No\" images to list\n",
    "    for img in glob.glob(os.getcwd() + \"\\\\brain_tumor_dataset\\\\no\\\\*.jpg\"):\n",
    "        n = cv2.imread(img)\n",
    "        # convert to gray scale\n",
    "        n = cv2.cvtColor(n, cv2.COLOR_BGR2GRAY)\n",
    "        # normalize data to range(0,1).\n",
    "        # In future - the normalization makes convergence faster while training the network.\n",
    "        n = cv2.normalize(n, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        # resize image - change dimensions -> from RGB(3 channels) to gray(1 channel)\n",
    "        resized = cv2.resize(n, dim, interpolation=cv2.INTER_AREA)\n",
    "        cv_img.append(resized)\n",
    "        cv_img_label.append(0)\n",
    "    # create tuple (images,labels)\n",
    "    tup_cv_image = (cv_img, cv_img_label)\n",
    "    return tup_cv_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(tup_cv_image):\n",
    "    # ----------------------flip horizontal----------------------\n",
    "    # create list to save images after perform flip horizontal\n",
    "    flip_horiz_images = []\n",
    "    flip_horiz_images_label = []\n",
    "    # iterate each image in original tuple and make Flipping\n",
    "    for img, label in zip(tup_cv_image[0],tup_cv_image[1]):\n",
    "        img = np.array(img)\n",
    "        flipped_img = np.fliplr(img)\n",
    "        flip_horiz_images.append(flipped_img)\n",
    "        flip_horiz_images_label.append(label)\n",
    "    # create tuple (images,labels)\n",
    "    tup_flip_horiz = (flip_horiz_images, flip_horiz_images_label)\n",
    "    # ----------------------add some noise----------------------\n",
    "    noise_images = []\n",
    "    noise_images_label =[]\n",
    "    for img, label in zip(tup_cv_image[0],tup_cv_image[1]):\n",
    "        noise = np.random.uniform(low=0, high=0.5, size=(128, 128))\n",
    "        new_img = np.zeros((128, 128))\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                if np.any(img[i][j] != 1):\n",
    "                    # adding noise\n",
    "                    new_img[i][j] = img[i][j] + noise[i][j]\n",
    "                    # range of pixel is between [0,1] , can't over 1 after adding noise.\n",
    "                    if new_img[i][j] > 1:\n",
    "                        new_img[i][j] = 1\n",
    "        noise_images.append(new_img)\n",
    "        noise_images_label.append(label)\n",
    "    # create tuple (images,labels)\n",
    "    tup_noise = (noise_images, noise_images_label)\n",
    "    # ----------------------flip vertical----------------------\n",
    "    flip_ver_images = []\n",
    "    flip_ver_images_label = []\n",
    "    for img, label in zip(tup_cv_image[0],tup_cv_image[1]):\n",
    "        flipped_ver_image = cv2.flip(img, 0)\n",
    "        flip_ver_images.append(flipped_ver_image)\n",
    "        flip_ver_images_label.append(label)\n",
    "    # create tuple (images,labels)\n",
    "    tup_flip_ver = (flip_ver_images,flip_ver_images_label)\n",
    "    # ----------------------concatenate all lists - create new dataset ----------------------\n",
    "    new_data_set = tup_cv_image[0] + tup_flip_horiz[0] + tup_noise[0] + tup_flip_ver[0]\n",
    "    new_data_set_label = tup_cv_image[1] + tup_flip_horiz[1] + tup_noise[1] + tup_flip_ver[1]\n",
    "    tup_new_dataset = (new_data_set, new_data_set_label)\n",
    "    return tup_new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(tup_new_dataset):\n",
    "    # split new dataset -> 80% train, 20% test\n",
    "    train, test, train_labels, test_labels = train_test_split(tup_new_dataset[0], tup_new_dataset[1],train_size=0.8)\n",
    "    test_before_changes = np.copy(test)\n",
    "    # convert to float because model.fit can't get np\n",
    "    train = np.float32(train)\n",
    "    train_labels = np.float32(train_labels)\n",
    "    test = np.float32(test)\n",
    "    test_labels = np.float32(test_labels)\n",
    "    # Reshape data for model.fit -> from (height, width, channel) change to (len, height, width, channel)\n",
    "    train = train.reshape(len(train), height, width, 1)\n",
    "    test = test.reshape(len(test), height, width, 1)\n",
    "    # build CNN model\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(128,128,1)))\n",
    "    model.add(layers.MaxPool2D())\n",
    "\n",
    "    model.add(layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D())\n",
    "\n",
    "    model.add(layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128,activation=\"relu\"))\n",
    "    model.add(layers.Dense(2, activation=\"softmax\"))\n",
    "    model.summary()\n",
    "    # compile and train the model\n",
    "    model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "    # early stopping is using in training and comes training to halt when there is an increment observed in loss values.\n",
    "    # earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\",mode=\"min\", patience=5, restore_best_weights=True)\n",
    "    # history = model.fit(train, train_labels, epochs=25, validation_data=(test, test_labels),callbacks=[earlystopping])\n",
    "    history = model.fit(train, train_labels, epochs=9, validation_data=(test, test_labels))\n",
    "    plt.figure()  # create a plot figure\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    test_loss, test_acc = model.evaluate(test, test_labels, verbose=2)\n",
    "    plt.show()\n",
    "    print(\"Accuracy  of the model is: \" + str(test_acc))\n",
    "    # save all images who get \"yes\" classification\n",
    "    # for each image in test we get two value , probability to be in class 0 and probability to be in class 1\n",
    "    # the sum of each row is 1\n",
    "    probas = model.predict(test)\n",
    "    # for each image return the index 0 or 1 which their probability is higher\n",
    "    classes = np.argmax(probas, axis=1)\n",
    "    index_img_in_test_class1 = [i for i, n in enumerate(classes) if n == 1]\n",
    "    images_test_class1 = test_before_changes[index_img_in_test_class1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first segmentation\n",
    "def threshold_seg(img):\n",
    "    ret, thresh3 = cv2.threshold(img, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "    cv2.imshow('Threshold', thresh3)\n",
    "    cv2.waitKey(0)\n",
    "    return thresh3/255\n",
    "\n",
    "\n",
    "# second segmentation\n",
    "def edge_based_seg(img):\n",
    "    edges = canny(img / 1.)\n",
    "    fill_img = ndi.binary_fill_holes(edges)\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.imshow(numpy.invert(fill_img), cmap=plt.cm.gray, interpolation='nearest')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Filling the holes')\n",
    "    plt.show()\n",
    "    return fill_img\n",
    "\n",
    "\n",
    "# three segmentation\n",
    "def region_based_seg(img):\n",
    "    elevation_map = sobel(img)\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    ax.imshow(elevation_map, cmap=plt.cm.gray, interpolation='nearest')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('elevation_map')\n",
    "    plt.show()\n",
    "    return elevation_map\n",
    "\n",
    "# This function is apply 3 types of segmentation and calculated Dice score for each technique.\n",
    "def dice():\n",
    "    target1 = cv2.imread('target_7.png',cv2.IMREAD_GRAYSCALE) / 255\n",
    "    img1 = cv2.imread('7.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    target2 = cv2.imread('target_145.png', cv2.IMREAD_GRAYSCALE) / 255\n",
    "    img2 = cv2.imread('145.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    target3 = cv2.imread('target_146.png', cv2.IMREAD_GRAYSCALE) / 255\n",
    "    img3 = cv2.imread('146.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    target4 = cv2.imread('target_149.png', cv2.IMREAD_GRAYSCALE) / 255\n",
    "    img4 = cv2.imread('149.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "    target5 = cv2.imread('target_150.png', cv2.IMREAD_GRAYSCALE) / 255\n",
    "    img5 = cv2.imread('150.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img1_threshold = threshold_seg(img1)\n",
    "    img1_edge = edge_based_seg(img1)\n",
    "    img1_region = region_based_seg(img1)\n",
    "\n",
    "    img2_threshold = threshold_seg(img2)\n",
    "    img2_edge = edge_based_seg(img2)\n",
    "    img2_region = region_based_seg(img2)\n",
    "\n",
    "    img3_threshold = threshold_seg(img3)\n",
    "    img3_edge = edge_based_seg(img3)\n",
    "    img3_region = region_based_seg(img3)\n",
    "\n",
    "    img4_threshold = threshold_seg(img4)\n",
    "    img4_edge = edge_based_seg(img4)\n",
    "    img4_region = region_based_seg(img4)\n",
    "\n",
    "    img5_threshold = threshold_seg(img5)\n",
    "    img5_edge = edge_based_seg(img5)\n",
    "    img5_region = region_based_seg(img5)\n",
    "\n",
    "\n",
    "    dice1_threshold = 2 * (np.sum(np.multiply(img1_threshold, target1))) / (np.sum(img1_threshold) + np.sum(target1))\n",
    "    dice1_edge = 2 * (np.sum(np.multiply(img1_edge, target1))) / (np.sum(img1_edge) + np.sum(target1))\n",
    "    dice1_region = 2 * (np.sum(np.multiply(img1_region, target1))) / (np.sum(img1_region) + np.sum(target1))\n",
    "\n",
    "    dice2_threshold = 2 * (np.sum(np.multiply(img2_threshold, target2))) / (np.sum(img2_threshold) + np.sum(target2))\n",
    "    dice2_edge = 2 * (np.sum(np.multiply(img2_edge, target2))) / (np.sum(img2_edge) + np.sum(target2))\n",
    "    dice2_region = 2 * (np.sum(np.multiply(img2_region, target2))) / (np.sum(img2_region) + np.sum(target2))\n",
    "\n",
    "    dice3_threshold = 2 * (np.sum(np.multiply(img3_threshold, target3))) / (np.sum(img3_threshold) + np.sum(target3))\n",
    "    dice3_edge = 2 * (np.sum(np.multiply(img3_edge, target3))) / (np.sum(img3_edge) + np.sum(target3))\n",
    "    dice3_region = 2 * (np.sum(np.multiply(img3_region, target3))) / (np.sum(img3_region) + np.sum(target3))\n",
    "\n",
    "    dice4_threshold = 2 * (np.sum(np.multiply(img4_threshold, target4))) / (np.sum(img4_threshold) + np.sum(target4))\n",
    "    dice4_edge = 2 * (np.sum(np.multiply(img4_edge, target4))) / (np.sum(img4_edge) + np.sum(target4))\n",
    "    dice4_region = 2 * (np.sum(np.multiply(img4_region, target4))) / (np.sum(img4_region) + np.sum(target4))\n",
    "\n",
    "    dice5_threshold = 2 * (np.sum(np.multiply(img5_threshold, target5))) / (np.sum(img5_threshold) + np.sum(target5))\n",
    "    dice5_edge = 2 * (np.sum(np.multiply(img5_edge, target5))) / (np.sum(img5_edge) + np.sum(target5))\n",
    "    dice5_region = 2 * (np.sum(np.multiply(img5_region, target5))) / (np.sum(img5_region) + np.sum(target5))\n",
    "\n",
    "    seg_type = ['img/type', 'Threshold', 'Edge based', 'Region based']\n",
    "    img_num = ['image 1','image 2','image 3','image 4','image 5']\n",
    "    dice_arr1 = [dice1_threshold,dice1_edge,dice1_region]\n",
    "    dice_arr2 = [dice2_threshold, dice2_edge, dice2_region]\n",
    "    dice_arr3 = [dice3_threshold, dice3_edge, dice3_region]\n",
    "    dice_arr4 = [dice4_threshold, dice4_edge, dice4_region]\n",
    "    dice_arr5 = [dice5_threshold, dice5_edge, dice5_region]\n",
    "\n",
    "    print('img 1:threshold,edge,region')\n",
    "    print(dice_arr1)\n",
    "    print('img 2:threshold,edge,region')\n",
    "    print(dice_arr2)\n",
    "    print('img 3:threshold,edge,region')\n",
    "    print(dice_arr3)\n",
    "    print('img 4:threshold,edge,region')\n",
    "    print(dice_arr4)\n",
    "    print('img 5:threshold,edge,region')\n",
    "    print(dice_arr5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f58ccda",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_13720/3107407018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtup_images_and_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_images_and_preprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtup_new_images_and_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_images_and_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mimages_test_class1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_new_images_and_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp/ipykernel_13720/1036459990.py\u001b[0m in \u001b[0;36mread_images_and_preprocessing\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcv_img_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# add \"yes\" images to list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\brain_tumor_dataset\\\\yes\\\\*.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# convert to gray scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tup_images_and_labels = read_images_and_preprocessing()\n",
    "    tup_new_images_and_labels = augmentation(tup_images_and_labels)\n",
    "    images_test_class1 = build_cnn_model(tup_new_images_and_labels)\n",
    "    dice()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb00f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9876b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
